# -*- coding: utf-8 -*-
"""ORISERVE_Assignment(Shailesh Pandit).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UTK9DzrVCsDpvyFjcRqnyGvclIfda16i

## Installing Sentence transformer from hugging-face for generatig sentance embeddings and text hero for text preprocessing
"""

!pip install texthero

!pip install -U sentence-transformers
#!pip install faiss

"""## Importing Important Packages """

# Commented out IPython magic to ensure Python compatibility.
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
import pandas as pd
from matplotlib import pyplot as plt
import numpy as np
import texthero as hero
# %matplotlib inline

"""## Downloading pretrained model for Sentence embedding generation 
SRC:https://huggingface.co/sentence-transformers/paraphrase-xlm-r-multilingual-v1 
"""

embedding_model=r'''sentence-transformers/paraphrase-xlm-r-multilingual-v1'''#r'''distilbert-base-nli-stsb-mean-tokens'''
embedder = SentenceTransformer(embedding_model)

"""## Reading the provided Text dataset and Removing all other empty columns from the pandas dataframe (Keeping only text column in data frame)"""

file_path=r'''https://github.com/shailesh897/Text-Based-Clustering/blob/506dda5975635b90a9bc26434b5750d6bf14a774/clustering.xlsx?raw=true
'''
data = pd.read_excel(file_path)
data=data['Text'].to_frame()
data.head()

data.size

"""## Removing Duplicate values"""

data = data.drop_duplicates('Text')
data_backup=data.copy()
data.size

"""##Preprocessing Of text
Removing URLS, digits, punctuations,Whitespace, Diacritics etc.

"""

import re
#this function is created for custum cleaning of given data
def custom_clean_text(x):
  x = " ".join(x.split())
  x=re.sub(r"\([^()]*\)", "", x)
  x = re.sub(r"\([^()]*\)", "", x)
  x = re.sub(r"(\*|\?|=)+", "", x) ##removing all *, ? and =
  x = re.sub(r"\b(\w+)( \1\b)+", r"\1", x) ## removing consecutive dupicate words
  x = x.replace("[REF]", "")
  x = x.replace("ref", "")
  x = re.sub(r"([^A-Za-z0-9\s](\s)){2,}", "", x)##remove consecutive punctuations
  return(x.replace("  ", " "))

data['Text']=data['Text'].apply(lambda x: custom_clean_text(x))

data['Text']=hero.remove_urls(data['Text'])
data['Text']=hero.remove_digits(data['Text'])
data['Text']=hero.remove_punctuation(data['Text'])
data['Text']=hero.remove_diacritics(data['Text'])
data['Text']=hero.remove_whitespace(data['Text'])
data['Text']=hero.lowercase(data['Text'])
data['Text']=hero.remove_stopwords(data['Text'])

data.head(50)

"""## Converting text to embeddings(786 dimension)"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# corpus=list(data['Text'])
# corpus_embeddings = embedder.encode(corpus)

corpus_embeddings.shape

corpus_embeddings

"""## This function could be useful for dimentionality reduction(SVD)"""

def reduce_dimension(embedding_df,new_dimension):
  from sklearn.decomposition import TruncatedSVD
  tsvd = TruncatedSVD(n_components=new_dimension)
  tsvd_3d = pd.DataFrame(tsvd.fit_transform(embedding_df))
  return tsvd_3d

"""## Reducing dimensions of embedding(to 2 dimension for visualisation)
This could lead to significant loss of information but  I am using this just to quickly validate the number of cluster [link text](https://) because using elbow method  on original embeddings will take time (Just to get idea about the range in which desired number of cluster will lie)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# reduced_embeddings=reduce_dimension(corpus_embeddings,2)

reduced_embeddings

"""## Using Elbow method for optimal numbers of cluster for the given dataset
 
 IMP: "**I am using  reduced embeddings**"
 It can clearly be seen that some where around cluster number 3 -  6,Decrease rate for WSS got slower.  It gives us the sense that optimal number of cluster should be near to this range.

## Plotting the ***WSS***(Within-Cluster-Sum of Squared Errors) vs K graph for defining elbow.
"""

sse = []
k_rng = range(1,20)
for k in k_rng:
    km = KMeans(n_clusters=k)
    km.fit(reduced_embeddings)
    sse.append(km.inertia_)
    print("WSS: ",km.inertia_,"  Clusters: ",k)

"""## Number of clusters =4 (seems right choice)"""

plt.xlabel('Number Of K')
plt.ylabel('WCSS')
plt.plot(k_rng,sse)
plt.scatter(k_rng,sse)#x="salary", y="school

"""## Validating number of cluster using **Silhouette Score**

SRC: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html

In the plot We see there is more negative observation with 4 clusters than 3 cluters
"""

#Runs for 8 Min on GPU
from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import numpy as np

X=reduced_embeddings
range_n_clusters=range(2,7)

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters, random_state=10)
    cluster_labels = clusterer.fit_predict(X)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(X, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

plt.show()

import sklearn.metrics as metrics

for i in range(2,30):
    labels=KMeans(n_clusters=i,init="k-means++",random_state=200).fit(reduced_embeddings).labels_
    print ("Silhouette score for k(clusters) = "+str(i)+" is "
           +str(metrics.silhouette_score(reduced_embeddings,labels,metric="euclidean",sample_size=1000,random_state=200)))

"""## Optimal number of cluster for the given dataset is **4**"""

NUM_OF_CLUSTERS=4

"""## Training the ***K Mean Algorithm*** with original sentence embeddings"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.cluster import KMeans
# 
# num_clusters = NUM_OF_CLUSTERS
# # Define kmeans model
# clustering_model = KMeans(n_clusters=num_clusters)
# 
# # Fitting the embedding with kmeans clustering.
# clustering_model.fit(corpus_embeddings)
# 
# # Get the cluster id assigned to each sentence.
# cluster_assignment = clustering_model.labels_

df_em=pd.DataFrame(reduced_embeddings)
df_em['Clusters']=clustering_model.labels_



import seaborn as sns
sns.scatterplot(data=df_em,x=df_em[0], y=df_em[1],hue='Clusters')

print(cluster_assignment)

data_Kmeans=data_backup.copy(deep=True)
data_Kmeans['cluster']=clustering_model.labels_
data_Kmeans.head(20)

data_Kmeans.tail(20)

"""## Saving The labeled dataset To excel sheet(CSV)"""

file_name="Labeled Data(KMean).csv"
data_Kmeans.to_csv(file_name)

# Commented out IPython magic to ensure Python compatibility.
# '''
# %%time
# from sklearn.cluster import AgglomerativeClustering
# 
# num_clusters = NUM_OF_CLUSTERS
# 
# A_clustering_model = AgglomerativeClustering(n_clusters=num_clusters)
# 
# 
# # Fitting the embedding with kmeans clustering.
# A_clustering_model.fit(reduced_embeddings)
# 
# # Get the cluster id assigned to each sentence.
# A_cluster_assignment = A_clustering_model.labels_
# '''